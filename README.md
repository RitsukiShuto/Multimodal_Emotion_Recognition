# Multimodal-Emotion-Recognition
### Created by RitsukiShuto

## インデックス
1. [特徴ベクトル作成](#1)
2. [プログラムの構成](#2)

# 1. 特徴ベクトルの作成
特徴ベクトルは、"Vectorization"ディレクトリの"vectrization.py"を使う。\
100行目~103行目のパラメータを変更することで特徴ベクトルの出力を変更することができる。\
各変数の説明は以下の通り。

- FILE_NUM = str(N)\
ファイル名の先頭につく識別番号

- LEN = N -> 特徴ベクトルに用いる発話文字数。\
初期値は0で任意の整数nが入力されたとき、n文字以下の発話を無視して特徴ベクトルの作成を行う。

- SOUND_DIM = n\
音声データの次元数。

- PICKUP_EMO = []\
学習に用いる感情レベルの選択。1~3は演技音声のそれぞれのLv.に対応しており、"9"は自発対話音声である。

## 学習データの扱い
ファイル名は"[番号]_meta_data.csv"のように命名している。
番号は以下の内容を示す。

- 音声64次元, テキスト367次元
1. 自発対話音声
2. 自発対話音声 + 演技音声(Lv. 1, 2)
3. 自発対話音声 + 演技音声(Lv. 2, 3)
4. 自発対話音声 + 演技音声(Lv. 1, 3)
5. 自発対話音声 + 演技音声(Lv. 1, 2, 3)
6. 自発対話音声 + 演技音声(Lv. 1)
7. 自発対話音声 + 演技音声(Lv. 2)
8. 自発対話音声 + 演技音声(Lv. 3)
9. 自発対話音声 + 演技音声(Lv. 1, 2, 3) + 5感情以外を示すラベルなしデータ

- 音声367次元, テキスト367次元
11. 自発対話音声 + 演技音声(Lv. 1, 2, 3)

# 2. プログラムの構成
## main.py
データの読み込みと各学習アルゴリズムへの分岐を行う。\

以下のプログラムを呼び出し、実行する。
1. supervised_learning()\
2. semi_supervised_learning()\
3. [DBG]supervised_learning()\
4. [DBG]semi_supervised_learning()\

3.4. の関数はデバッグ用であり学習を行わずに実行するものである。\
実験データなどのデータが正しく保存されていることを確かめるために使う。

## supervised_learning

## semi_supervised_learning

## models

## utils
